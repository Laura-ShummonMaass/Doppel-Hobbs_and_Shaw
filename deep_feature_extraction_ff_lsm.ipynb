{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "# # Required installations (run once)\n",
    "# !brew install wget  # Added by Miles\n",
    "# !pip install --upgrade --ignore-installed wrapt  # Added by Miles\n",
    "# !pip install tensorflow==2.0.0-beta0  # Edited by Miles (switch to CPU version)\n",
    "# !pip install tensorflow_datasets  # Added by Miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import pymongo        # MongoDB\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "### Data download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wvidPx6jeFzf"
   },
   "source": [
    "### Format the Data\n",
    "\n",
    "Use the `tf.image` module to format the images for the task.\n",
    "\n",
    "Resize the images to a fixes input size, and rescale the input channels to a range of `[-1,1]`\n",
    "\n",
    "<!-- TODO(markdaoust): fix the keras_applications preprocessing functions to work in tf2 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### _This (below) is used!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3PM6GVHcC31"
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 160 # All images will be resized to 160x160\n",
    "\n",
    "def prepare_image(image, height=IMG_SIZE, width=IMG_SIZE):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image/127.5) - 1\n",
    "    image = tf.image.resize(image, (height, width))\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### _Start custom image pull_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LICENSE                          \u001b[34mdrivers\u001b[m\u001b[m\r\n",
      "README.md                        fast_and_furious.py\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                      photos_of_ff_cast.ipynb\r\n",
      "aws_linked.ipynb                 photos_of_ff_cast_lsm.ipynb\r\n",
      "deep_feature_extraction_ff.ipynb \u001b[34mtest_data\u001b[m\u001b[m\r\n",
      "\u001b[34mdownloads\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['brian', 'hobbs']\n",
      "[]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-61989a637426>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mff\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mcurr_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirnames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/posixpath.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(a, *p)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mdiscarded\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mAn\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlast\u001b[0m \u001b[0mpart\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mpath\u001b[0m \u001b[0mthat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     ends with a separator.\"\"\"\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_sep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not list"
     ]
    }
   ],
   "source": [
    "# from os import walk\n",
    "\n",
    "f = []\n",
    "for dirpath, dirnames, filenames in os.walk('test_data'):\n",
    "    print(dirnames)\n",
    "    for ff in filenames:\n",
    "        curr_path = os.path.join(dirnames, ff)\n",
    "        print(curr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM Attempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['brian', 'hobbs'], [], []]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_names = []\n",
    "for dirpath, dirnames, filenames in os.walk('test_data'):\n",
    "    folder_names.append(dirnames)\n",
    "\n",
    "folder_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " ['6.paul-walker-medium.jpg',\n",
       "  '11.2.37830931.jpg',\n",
       "  '2.paul-walker-21044993-1-402.jpg',\n",
       "  '9.66892393.jpg',\n",
       "  '1.maxresdefault.jpg',\n",
       "  '12.gettyimages-168243951.jpg',\n",
       "  '5._102791014_gettyimages-164559639.jpg',\n",
       "  '8.paul-walker.jpg',\n",
       "  '3.220px-PaulWalkerEdit-1.jpg',\n",
       "  '4.MV5BMjIwODc0OTk2Nl5BMl5BanBnXkFtZTcwOTQ5MDA0Mg@@._V1_UX214_CR0,0,214,317_AL_.jpg'],\n",
       " ['3.dwayne-johnson-11818916-1-402.jpg',\n",
       "  '5.416x416.jpg',\n",
       "  '2.MV5BMTkyNDQ3NzAxM15BMl5BanBnXkFtZTgwODIwMTQ0NTE@._V1_.jpg',\n",
       "  '4.5b462c57f4af9c1a008b45eb-750-563.jpg',\n",
       "  '8.the-rock-1.jpg',\n",
       "  '7.190127_3871676_Dwayne__The_Rock__Johnson_Wants_To_Be_In__Cr_800x450_1432791619872.jpg',\n",
       "  '1.220px-Dwayne_Johnson_2%2C_2013.jpg',\n",
       "  '9.MV5BODYzNzg4MjAxMF5BMl5BanBnXkFtZTgwNDEyODQ0MzI@._CR792,139,275,275_UX402_UY402._SY201_SX201_AL_.jpg',\n",
       "  '10.Shutterstock-Dwayne-Johnson-JStone.jpg']]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = []\n",
    "for dirpath, dirnames, filenames in os.walk('test_data'):\n",
    "    file_names.append(filenames)\n",
    "\n",
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_names[1]:\n",
    "    image_paths.append('./test_data/' + folder_names[0][0] + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in file_names[2]:\n",
    "    image_paths.append('./test_data/' + folder_names[0][1] + '/' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test_data/brian/6.paul-walker-medium.jpg',\n",
       " './test_data/brian/11.2.37830931.jpg',\n",
       " './test_data/brian/2.paul-walker-21044993-1-402.jpg',\n",
       " './test_data/brian/9.66892393.jpg',\n",
       " './test_data/brian/1.maxresdefault.jpg',\n",
       " './test_data/brian/12.gettyimages-168243951.jpg',\n",
       " './test_data/brian/5._102791014_gettyimages-164559639.jpg',\n",
       " './test_data/brian/8.paul-walker.jpg',\n",
       " './test_data/brian/3.220px-PaulWalkerEdit-1.jpg',\n",
       " './test_data/brian/4.MV5BMjIwODc0OTk2Nl5BMl5BanBnXkFtZTcwOTQ5MDA0Mg@@._V1_UX214_CR0,0,214,317_AL_.jpg',\n",
       " './test_data/hobbs/3.dwayne-johnson-11818916-1-402.jpg',\n",
       " './test_data/hobbs/5.416x416.jpg',\n",
       " './test_data/hobbs/2.MV5BMTkyNDQ3NzAxM15BMl5BanBnXkFtZTgwODIwMTQ0NTE@._V1_.jpg',\n",
       " './test_data/hobbs/4.5b462c57f4af9c1a008b45eb-750-563.jpg',\n",
       " './test_data/hobbs/8.the-rock-1.jpg',\n",
       " './test_data/hobbs/7.190127_3871676_Dwayne__The_Rock__Johnson_Wants_To_Be_In__Cr_800x450_1432791619872.jpg',\n",
       " './test_data/hobbs/1.220px-Dwayne_Johnson_2%2C_2013.jpg',\n",
       " './test_data/hobbs/9.MV5BODYzNzg4MjAxMF5BMl5BanBnXkFtZTgwNDEyODQ0MzI@._CR792,139,275,275_UX402_UY402._SY201_SX201_AL_.jpg',\n",
       " './test_data/hobbs/10.Shutterstock-Dwayne-Johnson-JStone.jpg']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End LSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = '/Users/werlindo/Dropbox/flatiron/projects/mod4_project/downloads/_ready/dwayne_johnson/107.dwayne-johnson.jpg'\n",
    "# path2 = '/Users/werlindo/Dropbox/flatiron/projects/mod4_project/downloads/_ready/dwayne_johnson/104.intro-1525281779.jpg'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### _Get features._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "19IQ2gqneqmS",
    "outputId": "2659862d-e726-47b0-cdd9-355938312718"
   },
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"Return a vector of 1280 deep features for image.\"\"\"\n",
    "    image_resized = prepare_image(image)\n",
    "    image_np = image_resized.numpy()\n",
    "    images_np = np.expand_dims(image_np, axis=0)\n",
    "    image_np.shape, images_np.shape\n",
    "    deep_features = model.predict(images_np)\n",
    "    return deep_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "extract_features(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ImageFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts deep features from images.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"MobileNetV2\", height=160, width=160):\n",
    "        \"\"\"Creates an ImageFeatureExtractor using the specified model.\"\"\"\n",
    "        self.height, self.width = height, width\n",
    "        if model == \"MobileNetV2\":\n",
    "            base_model = tf.keras.applications.MobileNetV2(\n",
    "                input_shape=(height, width, 3),\n",
    "               include_top=False,\n",
    "               weights='imagenet'\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"Model unknown\")\n",
    "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.model = tf.keras.Sequential([base_model, global_average_layer])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"We're using a pre-trained model, so there's nothing to fit.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transforms image file paths into Numpy arrays of deep features.\"\"\"\n",
    "        result = []\n",
    "        for image_pathname in X:\n",
    "            result.append(self._transform_one(image_pathname))\n",
    "        print(len(result))\n",
    "#         return np.array(result)\n",
    "        return result\n",
    "    \n",
    "    def _transform_one(self, image_pathname):\n",
    "        \"\"\"Transforms a single image pathname into deep features.\"\"\"\n",
    "        img = Image.open(image_pathname)\n",
    "        img.load()\n",
    "        image = np.asarray(img)\n",
    "        return self._extract_features(image)\n",
    "    \n",
    "    def _extract_features(self, image):\n",
    "        \"\"\"Return a vector of 1280 deep features for image.\"\"\"\n",
    "        image_resized = prepare_image(image)\n",
    "        image_np = image_resized.numpy()\n",
    "        images_np = np.expand_dims(image_np, axis=0)\n",
    "        image_np.shape, images_np.shape\n",
    "        deep_features = self.model.predict(images_np)\n",
    "        return deep_features[0]\n",
    "    \n",
    "    def _prepare_image(self, image):\n",
    "        \"\"\"Converts an image to the expected format for prediction.\"\"\"\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image/127.5) - 1\n",
    "        image = tf.image.resize(image, (self.height, self.width))\n",
    "        return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0620 13:32:12.489801 4630705600 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/JonathanCMitchell/mobilenet_v2_keras/releases/download/v1.1/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_160_no_top.h5\n",
      "9412608/9406464 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "extractor = ImageFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-83-250cc397a56f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextractor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-6d5d3a302f50>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimage_pathname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_pathname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#         return np.array(result)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-6d5d3a302f50>\u001b[0m in \u001b[0;36m_transform_one\u001b[0;34m(self, image_pathname)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-6d5d3a302f50>\u001b[0m in \u001b[0;36m_extract_features\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;34m\"\"\"Return a vector of 1280 deep features for image.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mimage_resized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mimage_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_resized\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mimages_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_np\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mimage_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages_np\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'numpy'"
     ]
    }
   ],
   "source": [
    "features = extractor.transform(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/laurashummonmaass/Documents/flatiron/mod_4/final_project/mod4_project\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = dict(zip(img_paths, features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippy = list(zip(img_paths,features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippy[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zippy[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts = [{tup[0]:tup[1]} for tup in zippy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up connection string\n",
    "mongo_user = 'werlindo'\n",
    "mongo_pw = 'dsaf040119'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate client\n",
    "client = pymongo.MongoClient(\"mongodb+srv://\" + mongo_user + \":\" \n",
    "                         + mongo_pw \n",
    "                         + \"@dsaf-oy1s0.mongodb.net/test?retryWrites=true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cli = pymongo.MongoClient('mongodb+srv://werlindo:dsaf040119@dsaf-oy1s0.mongodb.net/test?retryWrites=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client['furious']\n",
    "coll = db['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coll.insert_many(list_of_dicts, bypass_document_validation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at DB names\n",
    "cur = client.list_databases()\n",
    "\n",
    "for item in cur:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at everything in our collection!\n",
    "cur = coll.find({})\n",
    "\n",
    "for item in cur:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### _We can do train / test split before we push through classification models?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = [(img.numpy(), label.numpy())\n",
    "                for (img, label) in raw_train.take(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [(img.numpy(), label.numpy())\n",
    "               for (img, label) in raw_test.take(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0][0])\n",
    "plt.title(get_label_name(train_images[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_img, train_y = zip(*train_images)\n",
    "test_X_img, test_y = zip(*test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LIMIT = 1000\n",
    "TEST_LIMIT = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_small = [extract_features(img)\n",
    "                 for img in train_X_img[:TRAIN_LIMIT]]\n",
    "test_X_small = [extract_features(img)\n",
    "                for img in test_X_img[:TEST_LIMIT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_small = train_y[:TRAIN_LIMIT]\n",
    "test_y_small = test_y[:TEST_LIMIT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on Deep Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(train_X_small, train_y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = lr.predict(train_X_small)\n",
    "(sum(train_y_small) / len(train_y_small),\n",
    " sum(train_preds == train_y_small) / len(train_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = lr.predict(test_X_small)\n",
    "(sum(test_y_small) / len(test_y_small),\n",
    " sum(test_preds == test_y_small) / len(test_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=np.argsort(np.abs(lr.coef_[0]))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'values': lr.coef_[0][idxs], 'idx': idxs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest on Deep Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "rfc.fit(train_X_small, train_y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = rfc.predict(train_X_small)\n",
    "(sum(train_y_small) / len(train_y_small),\n",
    " sum(train_preds == train_y_small) / len(train_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_X_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = rfc.predict(test_X_small)\n",
    "(sum(test_y_small) / len(test_y_small),\n",
    " sum(test_preds == test_y_small) / len(test_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_idxs = np.argsort(rfc.feature_importances_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_[rf_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_test_df = test_df.copy()\n",
    "idx_list = rf_idxs[:10]\n",
    "for idx in idx_list:\n",
    "    important_feature = test_df.loc[:, idx].copy().values\n",
    "    np.random.shuffle(important_feature)\n",
    "    permuted_test_df.loc[:, idx] = important_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = rfc.predict(permuted_test_df)\n",
    "(sum(test_y_small) / len(test_y_small),\n",
    " sum(test_preds == test_y_small) / len(test_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TqOt6Sv7AsMi"
   },
   "outputs": [],
   "source": [
    "# # Required installations (run once)\n",
    "# !brew install wget  # Added by Miles\n",
    "# !pip install --upgrade --ignore-installed wrapt  # Added by Miles\n",
    "# !pip install tensorflow==2.0.0-beta0  # Edited by Miles (switch to CPU version)\n",
    "# !pip install tensorflow_datasets  # Added by Miles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iBMcobPHdD8O"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "keras = tf.keras\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "import pymongo        # MongoDB\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mongo Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to secret\n",
    "secret_path = os.path.join(os.environ['HOME'], '.secret', 'mongo.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = helper.get_keys(secret_path)\n",
    "mongo_user = keys['user_id']\n",
    "mongo_pw = keys['password']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0GoKGm1duzgk"
   },
   "source": [
    "## Data download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./test_data/brian/6.paul-walker-medium.jpg',\n",
       " './test_data/brian/11.2.37830931.jpg',\n",
       " './test_data/brian/2.paul-walker-21044993-1-402.jpg',\n",
       " './test_data/brian/9.66892393.jpg',\n",
       " './test_data/brian/1.maxresdefault.jpg',\n",
       " './test_data/brian/12.gettyimages-168243951.jpg',\n",
       " './test_data/brian/5._102791014_gettyimages-164559639.jpg',\n",
       " './test_data/brian/8.paul-walker.jpg',\n",
       " './test_data/brian/3.220px-PaulWalkerEdit-1.jpg',\n",
       " './test_data/brian/4.MV5BMjIwODc0OTk2Nl5BMl5BanBnXkFtZTcwOTQ5MDA0Mg@@._V1_UX214_CR0,0,214,317_AL_.jpg',\n",
       " './test_data/hobbs/3.dwayne-johnson-11818916-1-402.jpg',\n",
       " './test_data/hobbs/5.416x416.jpg',\n",
       " './test_data/hobbs/2.MV5BMTkyNDQ3NzAxM15BMl5BanBnXkFtZTgwODIwMTQ0NTE@._V1_.jpg',\n",
       " './test_data/hobbs/4.5b462c57f4af9c1a008b45eb-750-563.jpg',\n",
       " './test_data/hobbs/8.the-rock-1.jpg',\n",
       " './test_data/hobbs/7.190127_3871676_Dwayne__The_Rock__Johnson_Wants_To_Be_In__Cr_800x450_1432791619872.jpg',\n",
       " './test_data/hobbs/1.220px-Dwayne_Johnson_2%2C_2013.jpg',\n",
       " './test_data/hobbs/9.MV5BODYzNzg4MjAxMF5BMl5BanBnXkFtZTgwNDEyODQ0MzI@._CR792,139,275,275_UX402_UY402._SY201_SX201_AL_.jpg',\n",
       " './test_data/hobbs/10.Shutterstock-Dwayne-Johnson-JStone.jpg']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from os import walk\n",
    "\n",
    "image_paths = []\n",
    "for dirpath, dirnames, filenames in os.walk('test_data'):\n",
    "    \n",
    "    for ff in filenames:\n",
    "        if ff[:1] != '.':\n",
    "            curr_path = os.path.join('.',dirpath, ff)\n",
    "#             print(curr_path)  \n",
    "            image_paths.append(curr_path)\n",
    "    \n",
    "image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "19IQ2gqneqmS",
    "outputId": "2659862d-e726-47b0-cdd9-355938312718"
   },
   "source": [
    "IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n",
    "\n",
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet')\n",
    "\n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "  base_model,\n",
    "  global_average_layer,\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def extract_features(image):\n",
    "    \"\"\"Return a vector of 1280 deep features for image.\"\"\"\n",
    "    image_resized = prepare_image(image)\n",
    "    image_np = image_resized.numpy()\n",
    "    images_np = np.expand_dims(image_np, axis=0)\n",
    "    image_np.shape, images_np.shape\n",
    "    deep_features = model.predict(images_np)\n",
    "    return deep_features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "extract_features(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class ImageFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extracts deep features from images.\"\"\"\n",
    "    \n",
    "    def __init__(self, model=\"MobileNetV2\", height=160, width=160):\n",
    "        \"\"\"Creates an ImageFeatureExtractor using the specified model.\"\"\"\n",
    "        self.height, self.width = height, width\n",
    "        if model == \"MobileNetV2\":\n",
    "            base_model = tf.keras.applications.MobileNetV2(\n",
    "                input_shape=(height, width, 3),\n",
    "               include_top=False,\n",
    "               weights='imagenet'\n",
    "            )\n",
    "        else:\n",
    "            raise Exception(\"Model unknown\")\n",
    "        global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "        self.model = tf.keras.Sequential([base_model, global_average_layer])\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"We're using a pre-trained model, so there's nothing to fit.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Transforms image file paths into Numpy arrays of deep features.\"\"\"\n",
    "        result = []\n",
    "        for image_pathname in X:\n",
    "            result.append(self._transform_one(image_pathname))\n",
    "#         print(len(result))\n",
    "#         return np.array(result)\n",
    "        return result\n",
    "    \n",
    "    def _transform_one(self, image_pathname):\n",
    "        \"\"\"Transforms a single image pathname into deep features.\"\"\"\n",
    "        img = Image.open(image_pathname)\n",
    "        img.load()\n",
    "        image = np.asarray(img)\n",
    "        return self._extract_features(image)\n",
    "    \n",
    "    def _extract_features(self, image):\n",
    "        \"\"\"Return a vector of 1280 deep features for image.\"\"\"\n",
    "        image_resized = prepare_image(image)\n",
    "        image_np = image_resized.numpy()\n",
    "        images_np = np.expand_dims(image_np, axis=0)\n",
    "        image_np.shape, images_np.shape\n",
    "        deep_features = self.model.predict(images_np)\n",
    "        return deep_features[0]\n",
    "    \n",
    "    def _prepare_image(self, image):\n",
    "        \"\"\"Converts an image to the expected format for prediction.\"\"\"\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image = (image/127.5) - 1\n",
    "        image = tf.image.resize(image, (self.height, self.width))\n",
    "        return image\n",
    "\n",
    "    def fetch_image_from_s3(self, bucket, key):             #Anna's Code\n",
    "        \"\"\"Fetches an image from S3 and returns a numpy array.\"\"\"\n",
    "        s3 = boto3.client('s3')\n",
    "        response = s3.get_object(Bucket=bucket, Key=key)\n",
    "        body = response['Body']\n",
    "        data = body.read()    \n",
    "        f = BytesIO(data)\n",
    "        image = Image.open(f)   \n",
    "        image_data = np.asarray(image)\n",
    "        return image_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Class\n",
    "extractor = ImageFeatureExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Store Features - list of arrays\n",
    "features = extractor.transform(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn into list of lists because easier with MongoDB\n",
    "features_list = [feature.tolist() for feature in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip them so can iterate through them\n",
    "zipped_imgs = list(zip(image_paths,features_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dictionaries; so can be ingested by MongoDB\n",
    "list_of_dicts = [{'url': img[0], 'features':img[1]} for img in zipped_imgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload results to MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate client\n",
    "client = pymongo.MongoClient(\"mongodb+srv://\" + mongo_user + \":\" \n",
    "                         + mongo_pw \n",
    "                         + \"@dsaf-oy1s0.mongodb.net/test?retryWrites=true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get DB, Collection\n",
    "db = client['furious']\n",
    "coll = db['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.DeleteResult at 0x157cb6b08>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wipe collection to start fresh\n",
    "coll.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x1502fbd48>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert Results\n",
    "coll.insert_many(list_of_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_returned = [np.array(x['features']) for x in coll.find()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.45510256, 0.        , 0.        , ..., 0.41784862, 0.        ,\n",
       "        0.01271284]),\n",
       " array([0., 0., 0., ..., 0., 0., 0.]),\n",
       " array([0.07392223, 0.        , 0.00697432, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.2498211 , 0.        , 0.17990871, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.93050528, 0.        , 0.83294296, ..., 0.09675258, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.16764668, 0.        , 0.        , ..., 0.00537357, 0.        ,\n",
       "        0.        ]),\n",
       " array([0.01346662, 0.        , 0.39356804, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.93678182, 0.2793051 , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.13647854, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.12628074, 0.        , 0.00428738, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.04088522, 0.        , 0.23422989, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.55403632, 1.69011509, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([2.08699489, 0.32195395, 0.        , ..., 0.        , 0.        ,\n",
       "        0.14281797]),\n",
       " array([0.45863277, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.65218794, 1.34712064, 0.        , ..., 0.        , 0.        ,\n",
       "        0.1053573 ]),\n",
       " array([0.26099759, 0.14074999, 0.        , ..., 0.04315201, 0.        ,\n",
       "        0.16554525]),\n",
       " array([0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.01504206]),\n",
       " array([0.02460576, 0.10758124, 0.00024496, ..., 0.        , 0.        ,\n",
       "        0.        ]),\n",
       " array([0.09389278, 0.66029853, 0.03010307, ..., 0.        , 0.        ,\n",
       "        0.04778893])]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_returned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSM Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = ['brian', 'brian', 'brian', 'brian', 'brian', 'brian', 'brian', 'brian', 'brian', 'brian', \n",
    "          'hobbs', 'hobbs', 'hobbs', 'hobbs', 'hobbs', 'hobbs', 'hobbs', 'hobbs', 'hobbs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=5,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf = DecisionTreeClassifier(criterion = \"gini\", max_depth = 5) \n",
    "tree_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       brian       1.00      0.33      0.50         3\n",
      "       hobbs       0.50      1.00      0.67         2\n",
      "\n",
      "   micro avg       0.60      0.60      0.60         5\n",
      "   macro avg       0.75      0.67      0.58         5\n",
      "weighted avg       0.80      0.60      0.57         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = tree_clf.predict(X_test)\n",
    "print(confusion_matrix(y_test, pred))\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=5, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=20, n_jobs=None,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest = RandomForestClassifier(n_estimators=20, max_depth= 5)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is likely not right.... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(features_returned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images.unsplash.com/photo-1518137569197-67c41a95d8de?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=2560&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graveyard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "### _We can do train / test split before we push through classification models?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = [(img.numpy(), label.numpy())\n",
    "                for (img, label) in raw_train.take(10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = [(img.numpy(), label.numpy())\n",
    "               for (img, label) in raw_test.take(1000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_images[0][0])\n",
    "plt.title(get_label_name(train_images[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_img, train_y = zip(*train_images)\n",
    "test_X_img, test_y = zip(*test_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract features from some images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_LIMIT = 1000\n",
    "TEST_LIMIT = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_small = [extract_features(img)\n",
    "                 for img in train_X_img[:TRAIN_LIMIT]]\n",
    "test_X_small = [extract_features(img)\n",
    "                for img in test_X_img[:TEST_LIMIT]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_small = train_y[:TRAIN_LIMIT]\n",
    "test_y_small = test_y[:TEST_LIMIT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression on Deep Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(train_X_small, train_y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = lr.predict(train_X_small)\n",
    "(sum(train_y_small) / len(train_y_small),\n",
    " sum(train_preds == train_y_small) / len(train_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = lr.predict(test_X_small)\n",
    "(sum(test_y_small) / len(test_y_small),\n",
    " sum(test_preds == test_y_small) / len(test_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs=np.argsort(np.abs(lr.coef_[0]))[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(lr.coef_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame({'values': lr.coef_[0][idxs], 'idx': idxs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest on Deep Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=-1)\n",
    "rfc.fit(train_X_small, train_y_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = rfc.predict(train_X_small)\n",
    "(sum(train_y_small) / len(train_y_small),\n",
    " sum(train_preds == train_y_small) / len(train_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(test_X_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = rfc.predict(test_X_small)\n",
    "(sum(test_y_small) / len(test_y_small),\n",
    " sum(test_preds == test_y_small) / len(test_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_idxs = np.argsort(rfc.feature_importances_)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.feature_importances_[rf_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permuted_test_df = test_df.copy()\n",
    "idx_list = rf_idxs[:10]\n",
    "for idx in idx_list:\n",
    "    important_feature = test_df.loc[:, idx].copy().values\n",
    "    np.random.shuffle(important_feature)\n",
    "    permuted_test_df.loc[:, idx] = important_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = rfc.predict(permuted_test_df)\n",
    "(sum(test_y_small) / len(test_y_small),\n",
    " sum(test_preds == test_y_small) / len(test_y_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at DB names\n",
    "cur = client.list_databases()\n",
    "\n",
    "for item in cur:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Look at everything in our collection!\n",
    "cur = coll.find({})\n",
    "\n",
    "for item in cur:\n",
    "    print(item)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "transfer_learning.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
